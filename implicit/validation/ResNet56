INFO: <function FlattenOpL.__init__.<locals>.<lambda> at 0x7fe41ced5a70> is not a torch Module
=> loading checkpoint '/home/beeperman/Projects/ImplicitLayers/implicit/validation/save_temp/ResNet56model.th'
=> loaded checkpoint 'False' (epoch 0)
Files already downloaded and verified
norm diff for the batch: 0.8491262793540955
Test: [0/79]	Time 597.682 (597.682)	Loss 0.3035 (0.3035)	Prec@1 94.531 (94.531)
norm diff for the batch: 0.7808387875556946
norm diff for the batch: 0.8268041610717773
norm diff for the batch: 0.7705764174461365
norm diff for the batch: 0.7572755813598633
norm diff for the batch: 0.8213461637496948
norm diff for the batch: 0.8482509851455688
norm diff for the batch: 0.7359616756439209
norm diff for the batch: 0.7820941209793091
norm diff for the batch: 0.7272207736968994
norm diff for the batch: 0.7494626641273499
norm diff for the batch: 0.7579967379570007
norm diff for the batch: 0.8452901840209961
norm diff for the batch: 0.8340800404548645
norm diff for the batch: 0.7309030890464783
norm diff for the batch: 0.8497484922409058
norm diff for the batch: 0.7985326051712036
norm diff for the batch: 0.78544682264328
norm diff for the batch: 0.7912247180938721
norm diff for the batch: 0.8509307503700256
norm diff for the batch: 0.8365787863731384
norm diff for the batch: 0.7941244840621948
norm diff for the batch: 0.8060173392295837
norm diff for the batch: 0.7960686087608337
norm diff for the batch: 0.8892245292663574
norm diff for the batch: 0.8085265755653381
norm diff for the batch: 0.7807221412658691
norm diff for the batch: 0.8097491264343262
norm diff for the batch: 0.7563372254371643
norm diff for the batch: 0.8112133741378784
norm diff for the batch: 0.7365782260894775
norm diff for the batch: 0.7728090286254883
norm diff for the batch: 0.7973004579544067
norm diff for the batch: 0.813334047794342
norm diff for the batch: 0.8315058946609497
norm diff for the batch: 0.8070508241653442
norm diff for the batch: 0.8172237277030945
norm diff for the batch: 0.8350231051445007
norm diff for the batch: 0.7852221131324768
norm diff for the batch: 0.8200998902320862
norm diff for the batch: 0.8408817648887634
norm diff for the batch: 0.8089198470115662
norm diff for the batch: 0.7658618092536926
norm diff for the batch: 0.8533262014389038
norm diff for the batch: 0.8192331790924072
norm diff for the batch: 0.8917080760002136
norm diff for the batch: 0.8428035378456116
norm diff for the batch: 0.8336069583892822
norm diff for the batch: 0.8090739250183105
norm diff for the batch: 0.8117552995681763
norm diff for the batch: 0.7929065823554993
Test: [50/79]	Time 562.295 (567.513)	Loss 0.2707 (0.3130)	Prec@1 92.188 (92.739)
norm diff for the batch: 0.7793362736701965
norm diff for the batch: 0.8396658897399902
norm diff for the batch: 0.833154559135437
norm diff for the batch: 0.8571141958236694
norm diff for the batch: 0.760888934135437
norm diff for the batch: 0.7758834958076477
norm diff for the batch: 0.824978768825531
norm diff for the batch: 0.8331087231636047
norm diff for the batch: 0.8278303742408752
norm diff for the batch: 0.8354471325874329
norm diff for the batch: 0.7690781354904175
norm diff for the batch: 0.8162102103233337
norm diff for the batch: 0.7980915307998657
norm diff for the batch: 0.831379771232605
norm diff for the batch: 0.7757634520530701
norm diff for the batch: 0.7969900369644165
norm diff for the batch: 0.7792373895645142
norm diff for the batch: 0.8196479082107544
norm diff for the batch: 0.7785624861717224
norm diff for the batch: 0.7968057990074158
norm diff for the batch: 0.7382034659385681
norm diff for the batch: 0.811164915561676
norm diff for the batch: 0.8309459686279297
norm diff for the batch: 0.8330946564674377
norm diff for the batch: 0.7685418725013733
norm diff for the batch: 0.813125491142273
norm diff for the batch: 0.8667775392532349
norm diff for the batch: 0.28120192885398865
 * Prec@1 93.010
